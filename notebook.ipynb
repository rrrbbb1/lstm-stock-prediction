{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import seaborn  as sns\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# openai.api_key = api_key\n",
    "# def analyze_sentiment(text):\n",
    "#     prompt = f\"Analyze the sentiment of the following text and classify it as Positive, Negative, or Neutral:\\n\\n\\\"{text}\\\"\"\n",
    "    \n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         max_tokens=50\n",
    "#     )\n",
    "    \n",
    "#     sentiment = response['choices'][0]['message']['content'].strip()\n",
    "#     return sentiment\n",
    "\n",
    "# text = news['content'].values[0]\n",
    "# sentiment = analyze_sentiment(text)\n",
    "# print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** No prenium APIKey for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_labelling(df):\n",
    "    \"\"\"\n",
    "    Perform sentiment labelling on the given DataFrame.\n",
    "    Adds 'negative', 'neutral', and 'positive' columns to the DataFrame.\n",
    "    Processes the DataFrame to group by date and fill missing values.\n",
    "    \"\"\"\n",
    "    # Initialize sentiment columns\n",
    "    df['negative'] = 0.0\n",
    "    df['neutral'] = 0.0\n",
    "    df['positive'] = 0.0\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Sentiment Analysis\", dynamic_ncols=True):\n",
    "        text = row['content']\n",
    "\n",
    "        # Tokenize the text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get predicted probabilities (softmax output)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "        # Assign probabilities to the corresponding columns\n",
    "        df.at[index, 'positive'] = probs[0]\n",
    "        df.at[index, 'negative'] = probs[1]\n",
    "        df.at[index, 'neutral'] = probs[2]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    if {'url', 'title', 'content'}.issubset(df.columns):\n",
    "        df = df.drop(columns=['url', 'title', 'content'])\n",
    "\n",
    "    # Group by date and calculate mean sentiment scores\n",
    "    df = df.groupby(\"date\").mean().reset_index()\n",
    "\n",
    "    # Ensure the 'date' column is in datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Create a full date range\n",
    "    date_range = pd.date_range(start=df[\"date\"].min(), end=df[\"date\"].max())\n",
    "\n",
    "    # Reindex to include all dates and forward-fill missing values\n",
    "    df = df.set_index(\"date\").reindex(date_range)\n",
    "    df = df.ffill().reset_index()\n",
    "    df.rename(columns={\"index\": \"date\"}, inplace=True)\n",
    "\n",
    "    # Sort by date in descending order\n",
    "    df = df.sort_values(by=\"date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_filtering(news, df):\n",
    "    # Ensure 'date' columns are in datetime format\n",
    "    news['date'] = pd.to_datetime(news['date'])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Find the intersection of dates\n",
    "    common_dates = set(news['date']).intersection(set(df['date']))\n",
    "\n",
    "    # Filter rows from both DataFrames where the date is in the common_dates\n",
    "    filtered_news = news[news['date'].isin(common_dates)]\n",
    "    filtered_df = df[df['date'].isin(common_dates)]\n",
    "    return filtered_news, filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixing_data(news, df, include_news=True, train_size=0.8, random_state=None):\n",
    "    \"\"\"\n",
    "    Combine company data with sentiment data, process features, and split into train-test sets.\n",
    "\n",
    "    Args:\n",
    "        news (pd.DataFrame): News sentiment data with 'date', 'negative', 'positive', and 'neutral' columns.\n",
    "        df (pd.DataFrame): Company stock data with 'Company', 'High', and 'date' columns.\n",
    "        include_news (bool): Whether to include sentiment data.\n",
    "        train_size (float): Proportion of data to use for training.\n",
    "        random_state (int, optional): Random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_data, test_data), where each is a NumPy array.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    companies = df['Company'].drop_duplicates().values\n",
    "    features = 400  # Number of companies to process\n",
    "    \n",
    "    # Process company data\n",
    "    for company in tqdm(companies[:features], desc=\"Processing company data\"):\n",
    "        company_data = df[df['Company'] == company]['High'].values\n",
    "        company_data = (company_data[1:] - company_data[:-1]) / company_data[:-1]  # Compute returns\n",
    "        data.append(company_data)\n",
    "\n",
    "    data = np.array(data)  # Shape: (features, timesteps)\n",
    "    \n",
    "    # Include sentiment data if specified\n",
    "    if include_news:\n",
    "        sentiment_data = []\n",
    "        dates = df['date'].drop_duplicates().values\n",
    "\n",
    "        for date in tqdm(dates[1:], desc=\"Processing sentiment data\"):\n",
    "            sentiment_data_day = news[news['date'] == date][['negative', 'positive', 'neutral']].values.flatten()\n",
    "            sentiment_data.append(sentiment_data_day)\n",
    "\n",
    "        sentiment_data = np.array(sentiment_data)  # Shape: (timesteps, sentiment_features)\n",
    "\n",
    "        # Check for timestep mismatch\n",
    "        if data.shape[1] != sentiment_data.shape[0]:\n",
    "            raise ValueError(f\"Mismatch in timesteps: company_data ({data.shape[1]}) vs sentiment_data ({sentiment_data.shape[0]})\")\n",
    "\n",
    "        sentiment_data = sentiment_data.T  # Shape: (sentiment_features, timesteps)\n",
    "        \n",
    "        # Combine company data with sentiment data\n",
    "        data = np.concatenate((data, sentiment_data), axis=0).T  # Shape: (timesteps, features + sentiment_features)\n",
    "\n",
    "    # Train-test split\n",
    "    train_data, test_data = train_test_split(\n",
    "        data,\n",
    "        train_size=train_size,\n",
    "        shuffle=False,  # Maintain temporal order\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, target_indices, sequence_length):\n",
    "        self.data = data # Shape must be : (data_length, num_features)\n",
    "        self.target_indices = target_indices  # Subset of features to predict\n",
    "        self.sequence_length = sequence_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.sequence_length, :]  # Input: All features\n",
    "        y = self.data[idx+self.sequence_length, self.target_indices]  # Target: Subset of features\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multivariate_data(train_data, test_data, target_index, sequence_length):\n",
    "    # Scale each feature independently\n",
    "    scalers = [StandardScaler() for _ in range(train_data.shape[1])]\n",
    "    train_scaled = np.zeros_like(train_data)\n",
    "    test_scaled = np.zeros_like(test_data)\n",
    "    \n",
    "    for i, scaler in enumerate(scalers):\n",
    "        train_scaled[:, i] = scaler.fit_transform(train_data[:, i].reshape(-1, 1)).flatten()\n",
    "        test_scaled[:, i] = scaler.transform(test_data[:, i].reshape(-1, 1)).flatten()\n",
    "    \n",
    "    train_dataset = MultivariateTimeSeriesDataset(train_scaled, target_index, sequence_length)\n",
    "    test_dataset = MultivariateTimeSeriesDataset(test_scaled, target_index, sequence_length)\n",
    "    \n",
    "    return train_dataset, test_dataset, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(train_dataset, test_dataset, batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scalers, target_indices):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            y_true.append(batch_y.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    \n",
    "    # Inverse transform predictions and ground truth for the target features\n",
    "    y_true_rescaled, y_pred_rescaled = [], []\n",
    "    for i, target_index in enumerate(target_indices):\n",
    "        scaler = scalers[target_index]\n",
    "        y_true_rescaled.append(scaler.inverse_transform(y_true[:, i].reshape(-1, 1)).flatten())\n",
    "        y_pred_rescaled.append(scaler.inverse_transform(y_pred[:, i].reshape(-1, 1)).flatten())\n",
    "    \n",
    "    y_true_rescaled = np.stack(y_true_rescaled, axis=1)\n",
    "    y_pred_rescaled = np.stack(y_pred_rescaled, axis=1)\n",
    "    \n",
    "    # Compute R2 scores for each feature\n",
    "    r2_scores = [r2_score(y_true_rescaled[:, i], y_pred_rescaled[:, i]) for i in range(len(target_indices))]\n",
    "    for i, score in enumerate(r2_scores):\n",
    "        print(f\"R2 Score for Feature {target_indices[i]}: {score:.4f}\")\n",
    "    return r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, early_stopping=None):\n",
    "    \"\"\"\n",
    "    Train a model with training and validation phases, and optional early stopping.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        epochs (int): Number of epochs to train.\n",
    "        early_stopping (EarlyStopping, optional): Early stopping instance to monitor validation loss.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch  # Inputs and targets already properly structured\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)  # Forward pass\n",
    "            loss = criterion(y_pred, y_batch)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Optimization step\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, y_batch = batch  # Inputs and targets already properly structured\n",
    "                y_pred = model(x_batch)  # Forward pass\n",
    "                loss = criterion(y_pred, y_batch)  # Compute loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Log the losses\n",
    "        print(f\"\\nEpoch {epoch+1}, Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Early stopping (if provided)\n",
    "        if early_stopping:\n",
    "            early_stopping(val_loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"\\n     ###################     \\n Early stopping triggered \\n     ###################     \")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scalers, target_indices):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            y_true.append(batch_y.numpy())\n",
    "            y_pred.append(outputs.numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "    \n",
    "    # Inverse transform predictions and ground truth for the target features\n",
    "    y_true_rescaled, y_pred_rescaled = [], []\n",
    "    for i, target_index in enumerate(target_indices):\n",
    "        scaler = scalers[target_index]\n",
    "        y_true_rescaled.append(scaler.inverse_transform(y_true[:, i].reshape(-1, 1)).flatten())\n",
    "        y_pred_rescaled.append(scaler.inverse_transform(y_pred[:, i].reshape(-1, 1)).flatten())\n",
    "    \n",
    "    y_true_rescaled = np.stack(y_true_rescaled, axis=1)\n",
    "    y_pred_rescaled = np.stack(y_pred_rescaled, axis=1)\n",
    "    \n",
    "    # Compute R2 scores for each feature\n",
    "    r2_scores = [r2_score(y_true_rescaled[:, i], y_pred_rescaled[:, i]) for i in range(len(target_indices))]\n",
    "    for i, score in enumerate(r2_scores):\n",
    "        print(f\"R2 Score for Feature {target_indices[i]}: {score:.4f}\")\n",
    "    return r2_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained different models using two types of data:\n",
    "- 1: without including news sentiment analysis\n",
    "- 2: including news sentiment analysis\n",
    "\n",
    "---> Maybe a bit dumb: the more you include data, the more your model is precise\n",
    "\n",
    "---> Strongly depends of the quality of the datasets of the news and the quality of the prediction by the FinBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the data used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we include sentiment data, we add three columns: positive, negative, neutral that describes to three different sentiments for the stock movement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess `df`\n",
    "df = pd.read_csv('data/sp500_prices.csv')\n",
    "df['date'] = pd.to_datetime(df['Date'])\n",
    "df = df.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess `news`\n",
    "news = pd.read_csv('data/bis_press_releases.csv')\n",
    "news = news.dropna()\n",
    "news['date'] = pd.to_datetime(news['date'], format='%d %b %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Time Series Forecasting using Sentiment Analysis with FinBERT and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess `news`\n",
    "news = pd.read_csv('data/bis_press_releases.csv')\n",
    "news = news.dropna()\n",
    "news['date'] = pd.to_datetime(news['date'], format='%d %b %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess `df`\n",
    "df = pd.read_csv('data/sp500_prices.csv')\n",
    "df['date'] = pd.to_datetime(df['Date'])\n",
    "df = df.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: neutral\n",
      "Probabilities: tensor([[0.1161, 0.2905, 0.5935]])\n",
      "labels assignation: {'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "class_labels = model.config.id2label\n",
    "prob_labels = model.config.label2id\n",
    "\n",
    "text1 = news['content'].values[0]\n",
    "inputs = tokenizer(text1, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():  # No gradient calculation needed for predictions\n",
    "    outputs = model(**inputs)\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "# Print the results\n",
    "print(\"Predicted class:\", class_labels[predicted_class])\n",
    "print(\"Probabilities:\", probs)\n",
    "print('labels assignation:', prob_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentiment Analysis: 100%|██████████| 7057/7057 [16:48<00:00,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "news = sentiment_labelling(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "news, df = dates_filtering(news, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing company data: 100%|██████████| 400/400 [00:08<00:00, 49.33it/s]\n",
      "Processing sentiment data: 100%|██████████| 1992/1992 [00:00<00:00, 5758.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = mixing_data(news, df, include_news=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "target_indices = [k for k in range(401)]\n",
    "\n",
    "train_dataset, test_dataset, scalers = load_multivariate_data(train_data, test_data, target_indices, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, intermediate_size=256, dropout=0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(intermediate_size, intermediate_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.fc3 = nn.Linear(intermediate_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states to zeros\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)  # Hidden state\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)  # Cell state\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take the last time step's output\n",
    "        out = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layer to produce the final output\n",
    "        out = self.fc1(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out) # Shape: (batch_size, output_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, path='checkpoint.pt', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement.\n",
    "            delta (float): Minimum change to qualify as an improvement.\n",
    "            path (str): Path to save the best model.\n",
    "            verbose (bool): Print messages about early stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Save the model when validation loss improves.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased to {val_loss:.6f}. Saving model...\")\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remibreton/miniconda3/envs/my_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_size = train_data.shape[1]\n",
    "output_size = len(target_indices)\n",
    "\n",
    "# Hyperparameters for the structure of the model\n",
    "hidden_size = 128\n",
    "num_layers = 6\n",
    "dropout = 0.0\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.02\n",
    "num_epochs = 200\n",
    "\n",
    "# Initialize the LSTM model\n",
    "model = LSTM(input_size, hidden_size, output_size, num_layers, dropout=dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "early_stopping = EarlyStopping(patience=20, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Train Loss: 1.619245, Validation Loss: 0.800944\n",
      "Validation loss decreased to 0.800944. Saving model...\n",
      "\n",
      "Epoch 2, Train Loss: 1.002031, Validation Loss: 0.799706\n",
      "Validation loss decreased to 0.799706. Saving model...\n",
      "\n",
      "Epoch 3, Train Loss: 1.009764, Validation Loss: 0.799965\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "Epoch 4, Train Loss: 1.000138, Validation Loss: 0.799462\n",
      "Validation loss decreased to 0.799462. Saving model...\n",
      "\n",
      "Epoch 5, Train Loss: 1.007853, Validation Loss: 0.799663\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "Epoch 6, Train Loss: 1.002212, Validation Loss: 0.799739\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "Epoch 7, Train Loss: 1.000102, Validation Loss: 0.799702\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "Epoch 8, Train Loss: 1.006043, Validation Loss: 0.799523\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "Epoch 9, Train Loss: 1.007469, Validation Loss: 0.799409\n",
      "Validation loss decreased to 0.799409. Saving model...\n",
      "\n",
      "Epoch 10, Train Loss: 1.014396, Validation Loss: 0.799625\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "Epoch 11, Train Loss: 1.012924, Validation Loss: 0.799454\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "Epoch 12, Train Loss: 1.008652, Validation Loss: 0.799475\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "Epoch 13, Train Loss: 1.009721, Validation Loss: 0.799750\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "Epoch 14, Train Loss: 1.001714, Validation Loss: 0.799739\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "Epoch 15, Train Loss: 1.011950, Validation Loss: 0.799567\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "Epoch 16, Train Loss: 1.012933, Validation Loss: 0.799650\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "Epoch 17, Train Loss: 1.006660, Validation Loss: 0.800174\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "Epoch 18, Train Loss: 1.013513, Validation Loss: 0.799562\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "Epoch 19, Train Loss: 1.002820, Validation Loss: 0.799773\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "Epoch 20, Train Loss: 1.004991, Validation Loss: 0.799458\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "Epoch 21, Train Loss: 1.007912, Validation Loss: 0.800275\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "Epoch 22, Train Loss: 1.005095, Validation Loss: 0.799874\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "Epoch 23, Train Loss: 1.007599, Validation Loss: 0.799609\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "Epoch 24, Train Loss: 1.003073, Validation Loss: 0.799860\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "Epoch 25, Train Loss: 1.003485, Validation Loss: 0.800136\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "Epoch 26, Train Loss: 1.001770, Validation Loss: 0.800087\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "Epoch 27, Train Loss: 1.007248, Validation Loss: 0.799453\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "Epoch 28, Train Loss: 1.009576, Validation Loss: 0.799748\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "Epoch 29, Train Loss: 1.012767, Validation Loss: 0.799653\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "     ###################     \n",
      " Early stopping triggered \n",
      "     ###################     \n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=test_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    epochs=epochs, \n",
    "    early_stopping=early_stopping\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score for Feature 0: -0.0077\n",
      "R2 Score for Feature 1: -0.0003\n",
      "R2 Score for Feature 2: -0.0026\n",
      "R2 Score for Feature 3: -0.0001\n",
      "R2 Score for Feature 4: -0.0008\n",
      "R2 Score for Feature 5: -0.0026\n",
      "R2 Score for Feature 6: -0.0004\n",
      "R2 Score for Feature 7: -0.0003\n",
      "R2 Score for Feature 8: -0.0003\n",
      "R2 Score for Feature 9: -0.0001\n",
      "R2 Score for Feature 10: -0.0005\n",
      "R2 Score for Feature 11: -0.0087\n",
      "R2 Score for Feature 12: -0.0004\n",
      "R2 Score for Feature 13: -0.0087\n",
      "R2 Score for Feature 14: -0.0001\n",
      "R2 Score for Feature 15: -0.0008\n",
      "R2 Score for Feature 16: -0.0001\n",
      "R2 Score for Feature 17: -0.0031\n",
      "R2 Score for Feature 18: -0.0000\n",
      "R2 Score for Feature 19: -0.0024\n",
      "R2 Score for Feature 20: -0.0000\n",
      "R2 Score for Feature 21: -0.0004\n",
      "R2 Score for Feature 22: -0.0011\n",
      "R2 Score for Feature 23: -0.0015\n",
      "R2 Score for Feature 24: -0.0071\n",
      "R2 Score for Feature 25: -0.0006\n",
      "R2 Score for Feature 26: -0.0038\n",
      "R2 Score for Feature 27: -0.0073\n",
      "R2 Score for Feature 28: -0.0067\n",
      "R2 Score for Feature 29: -0.0000\n",
      "R2 Score for Feature 30: -0.0057\n",
      "R2 Score for Feature 31: -0.0056\n",
      "R2 Score for Feature 32: -0.0013\n",
      "R2 Score for Feature 33: -0.0015\n",
      "R2 Score for Feature 34: -0.0030\n",
      "R2 Score for Feature 35: -0.0053\n",
      "R2 Score for Feature 36: -0.0000\n",
      "R2 Score for Feature 37: -0.0001\n",
      "R2 Score for Feature 38: -0.0069\n",
      "R2 Score for Feature 39: -0.0026\n",
      "R2 Score for Feature 40: -0.0000\n",
      "R2 Score for Feature 41: -0.0013\n",
      "R2 Score for Feature 42: -0.0027\n",
      "R2 Score for Feature 43: -0.0000\n",
      "R2 Score for Feature 44: -0.0010\n",
      "R2 Score for Feature 45: -0.0004\n",
      "R2 Score for Feature 46: -0.0143\n",
      "R2 Score for Feature 47: -0.0010\n",
      "R2 Score for Feature 48: -0.0001\n",
      "R2 Score for Feature 49: -0.0015\n",
      "R2 Score for Feature 50: -0.0020\n",
      "R2 Score for Feature 51: -0.0111\n",
      "R2 Score for Feature 52: -0.0029\n",
      "R2 Score for Feature 53: -0.0073\n",
      "R2 Score for Feature 54: -0.0000\n",
      "R2 Score for Feature 55: -0.0001\n",
      "R2 Score for Feature 56: -0.0027\n",
      "R2 Score for Feature 57: -0.0020\n",
      "R2 Score for Feature 58: -0.0001\n",
      "R2 Score for Feature 59: -0.0015\n",
      "R2 Score for Feature 60: -0.0032\n",
      "R2 Score for Feature 61: -0.0043\n",
      "R2 Score for Feature 62: -0.0080\n",
      "R2 Score for Feature 63: -0.0016\n",
      "R2 Score for Feature 64: -0.0005\n",
      "R2 Score for Feature 65: -0.0054\n",
      "R2 Score for Feature 66: -0.0005\n",
      "R2 Score for Feature 67: -0.0196\n",
      "R2 Score for Feature 68: -0.0080\n",
      "R2 Score for Feature 69: -0.0096\n",
      "R2 Score for Feature 70: -0.0051\n",
      "R2 Score for Feature 71: -0.0008\n",
      "R2 Score for Feature 72: -0.0020\n",
      "R2 Score for Feature 73: -0.0002\n",
      "R2 Score for Feature 74: -0.0000\n",
      "R2 Score for Feature 75: -0.0015\n",
      "R2 Score for Feature 76: -0.0018\n",
      "R2 Score for Feature 77: -0.0016\n",
      "R2 Score for Feature 78: -0.0015\n",
      "R2 Score for Feature 79: -0.0020\n",
      "R2 Score for Feature 80: -0.0061\n",
      "R2 Score for Feature 81: -0.0000\n",
      "R2 Score for Feature 82: -0.0043\n",
      "R2 Score for Feature 83: -0.0011\n",
      "R2 Score for Feature 84: -0.0006\n",
      "R2 Score for Feature 85: -0.0355\n",
      "R2 Score for Feature 86: -0.0003\n",
      "R2 Score for Feature 87: -0.0074\n",
      "R2 Score for Feature 88: -0.0024\n",
      "R2 Score for Feature 89: -0.0032\n",
      "R2 Score for Feature 90: -0.0099\n",
      "R2 Score for Feature 91: -0.0014\n",
      "R2 Score for Feature 92: -0.0002\n",
      "R2 Score for Feature 93: -0.0053\n",
      "R2 Score for Feature 94: -0.0000\n",
      "R2 Score for Feature 95: -0.0034\n",
      "R2 Score for Feature 96: -0.0009\n",
      "R2 Score for Feature 97: -0.0071\n",
      "R2 Score for Feature 98: -0.0003\n",
      "R2 Score for Feature 99: -0.0010\n",
      "R2 Score for Feature 100: -0.0047\n",
      "R2 Score for Feature 101: -0.0004\n",
      "R2 Score for Feature 102: -0.0002\n",
      "R2 Score for Feature 103: -0.0000\n",
      "R2 Score for Feature 104: -0.0006\n",
      "R2 Score for Feature 105: -0.0077\n",
      "R2 Score for Feature 106: -0.0010\n",
      "R2 Score for Feature 107: -0.0019\n",
      "R2 Score for Feature 108: -0.0014\n",
      "R2 Score for Feature 109: -0.0024\n",
      "R2 Score for Feature 110: -0.0000\n",
      "R2 Score for Feature 111: -0.0026\n",
      "R2 Score for Feature 112: -0.0011\n",
      "R2 Score for Feature 113: -0.0086\n",
      "R2 Score for Feature 114: -0.0026\n",
      "R2 Score for Feature 115: -0.0000\n",
      "R2 Score for Feature 116: -0.0004\n",
      "R2 Score for Feature 117: -0.0030\n",
      "R2 Score for Feature 118: -0.0065\n",
      "R2 Score for Feature 119: -0.0042\n",
      "R2 Score for Feature 120: -0.0050\n",
      "R2 Score for Feature 121: -0.0006\n",
      "R2 Score for Feature 122: -0.0000\n",
      "R2 Score for Feature 123: -0.0002\n",
      "R2 Score for Feature 124: -0.0079\n",
      "R2 Score for Feature 125: -0.0001\n",
      "R2 Score for Feature 126: -0.0026\n",
      "R2 Score for Feature 127: -0.0068\n",
      "R2 Score for Feature 128: -0.0107\n",
      "R2 Score for Feature 129: -0.0011\n",
      "R2 Score for Feature 130: -0.0141\n",
      "R2 Score for Feature 131: -0.0003\n",
      "R2 Score for Feature 132: -0.0017\n",
      "R2 Score for Feature 133: -0.0024\n",
      "R2 Score for Feature 134: -0.0032\n",
      "R2 Score for Feature 135: -0.0035\n",
      "R2 Score for Feature 136: -0.0001\n",
      "R2 Score for Feature 137: -0.0016\n",
      "R2 Score for Feature 138: -0.0066\n",
      "R2 Score for Feature 139: -0.0039\n",
      "R2 Score for Feature 140: -0.0018\n",
      "R2 Score for Feature 141: -0.0009\n",
      "R2 Score for Feature 142: -0.0044\n",
      "R2 Score for Feature 143: -0.0024\n",
      "R2 Score for Feature 144: -0.0044\n",
      "R2 Score for Feature 145: -0.0074\n",
      "R2 Score for Feature 146: -0.0000\n",
      "R2 Score for Feature 147: -0.0002\n",
      "R2 Score for Feature 148: -0.0016\n",
      "R2 Score for Feature 149: -0.0030\n",
      "R2 Score for Feature 150: -0.0019\n",
      "R2 Score for Feature 151: -0.0032\n",
      "R2 Score for Feature 152: -0.0026\n",
      "R2 Score for Feature 153: -0.0028\n",
      "R2 Score for Feature 154: -0.0037\n",
      "R2 Score for Feature 155: -0.0066\n",
      "R2 Score for Feature 156: -0.0008\n",
      "R2 Score for Feature 157: -0.0016\n",
      "R2 Score for Feature 158: -0.0002\n",
      "R2 Score for Feature 159: -0.0220\n",
      "R2 Score for Feature 160: -0.0036\n",
      "R2 Score for Feature 161: -0.0032\n",
      "R2 Score for Feature 162: -0.0044\n",
      "R2 Score for Feature 163: -0.0024\n",
      "R2 Score for Feature 164: -0.0020\n",
      "R2 Score for Feature 165: -0.0017\n",
      "R2 Score for Feature 166: -0.0004\n",
      "R2 Score for Feature 167: -0.0001\n",
      "R2 Score for Feature 168: -0.0058\n",
      "R2 Score for Feature 169: -0.0006\n",
      "R2 Score for Feature 170: -0.0049\n",
      "R2 Score for Feature 171: -0.0001\n",
      "R2 Score for Feature 172: -0.0139\n",
      "R2 Score for Feature 173: -0.0049\n",
      "R2 Score for Feature 174: -0.0047\n",
      "R2 Score for Feature 175: -0.0003\n",
      "R2 Score for Feature 176: -0.0040\n",
      "R2 Score for Feature 177: -0.0003\n",
      "R2 Score for Feature 178: -0.0021\n",
      "R2 Score for Feature 179: -0.0021\n",
      "R2 Score for Feature 180: -0.0022\n",
      "R2 Score for Feature 181: -0.0054\n",
      "R2 Score for Feature 182: -0.0000\n",
      "R2 Score for Feature 183: -0.0001\n",
      "R2 Score for Feature 184: -0.0017\n",
      "R2 Score for Feature 185: -0.0001\n",
      "R2 Score for Feature 186: -0.0234\n",
      "R2 Score for Feature 187: -0.0083\n",
      "R2 Score for Feature 188: -0.0084\n",
      "R2 Score for Feature 189: -0.0011\n",
      "R2 Score for Feature 190: -0.0001\n",
      "R2 Score for Feature 191: -0.0115\n",
      "R2 Score for Feature 192: -0.0001\n",
      "R2 Score for Feature 193: -0.0122\n",
      "R2 Score for Feature 194: -0.0000\n",
      "R2 Score for Feature 195: -0.0050\n",
      "R2 Score for Feature 196: -0.0001\n",
      "R2 Score for Feature 197: -0.0077\n",
      "R2 Score for Feature 198: -0.0007\n",
      "R2 Score for Feature 199: -0.0006\n",
      "R2 Score for Feature 200: -0.0001\n",
      "R2 Score for Feature 201: -0.0003\n",
      "R2 Score for Feature 202: -0.0009\n",
      "R2 Score for Feature 203: -0.0013\n",
      "R2 Score for Feature 204: -0.0014\n",
      "R2 Score for Feature 205: -0.0018\n",
      "R2 Score for Feature 206: -0.0006\n",
      "R2 Score for Feature 207: -0.0004\n",
      "R2 Score for Feature 208: -0.0004\n",
      "R2 Score for Feature 209: -0.0027\n",
      "R2 Score for Feature 210: -0.0143\n",
      "R2 Score for Feature 211: -0.0006\n",
      "R2 Score for Feature 212: -0.0000\n",
      "R2 Score for Feature 213: -0.0000\n",
      "R2 Score for Feature 214: -0.0030\n",
      "R2 Score for Feature 215: -0.0000\n",
      "R2 Score for Feature 216: -0.0010\n",
      "R2 Score for Feature 217: -0.0004\n",
      "R2 Score for Feature 218: -0.0017\n",
      "R2 Score for Feature 219: -0.0016\n",
      "R2 Score for Feature 220: -0.0038\n",
      "R2 Score for Feature 221: -0.0026\n",
      "R2 Score for Feature 222: -0.0001\n",
      "R2 Score for Feature 223: -0.0019\n",
      "R2 Score for Feature 224: -0.0004\n",
      "R2 Score for Feature 225: -0.0000\n",
      "R2 Score for Feature 226: -0.0024\n",
      "R2 Score for Feature 227: -0.0001\n",
      "R2 Score for Feature 228: -0.0001\n",
      "R2 Score for Feature 229: -0.0001\n",
      "R2 Score for Feature 230: -0.0071\n",
      "R2 Score for Feature 231: -0.0002\n",
      "R2 Score for Feature 232: -0.0053\n",
      "R2 Score for Feature 233: -0.0021\n",
      "R2 Score for Feature 234: -0.0047\n",
      "R2 Score for Feature 235: -0.0002\n",
      "R2 Score for Feature 236: -0.0079\n",
      "R2 Score for Feature 237: -0.0143\n",
      "R2 Score for Feature 238: -0.0001\n",
      "R2 Score for Feature 239: -0.0055\n",
      "R2 Score for Feature 240: -0.0032\n",
      "R2 Score for Feature 241: -0.0004\n",
      "R2 Score for Feature 242: -0.0031\n",
      "R2 Score for Feature 243: -0.0032\n",
      "R2 Score for Feature 244: -0.0006\n",
      "R2 Score for Feature 245: -0.0116\n",
      "R2 Score for Feature 246: -0.0018\n",
      "R2 Score for Feature 247: -0.0032\n",
      "R2 Score for Feature 248: -0.0006\n",
      "R2 Score for Feature 249: -0.0010\n",
      "R2 Score for Feature 250: -0.0040\n",
      "R2 Score for Feature 251: -0.0000\n",
      "R2 Score for Feature 252: -0.0037\n",
      "R2 Score for Feature 253: -0.0135\n",
      "R2 Score for Feature 254: -0.0010\n",
      "R2 Score for Feature 255: -0.0106\n",
      "R2 Score for Feature 256: -0.0017\n",
      "R2 Score for Feature 257: -0.0018\n",
      "R2 Score for Feature 258: -0.0014\n",
      "R2 Score for Feature 259: -0.0097\n",
      "R2 Score for Feature 260: -0.0051\n",
      "R2 Score for Feature 261: -0.0041\n",
      "R2 Score for Feature 262: -0.0002\n",
      "R2 Score for Feature 263: -0.0112\n",
      "R2 Score for Feature 264: -0.0005\n",
      "R2 Score for Feature 265: -0.0000\n",
      "R2 Score for Feature 266: -0.0012\n",
      "R2 Score for Feature 267: -0.0169\n",
      "R2 Score for Feature 268: -0.0142\n",
      "R2 Score for Feature 269: -0.0011\n",
      "R2 Score for Feature 270: -0.0005\n",
      "R2 Score for Feature 271: -0.0006\n",
      "R2 Score for Feature 272: -0.0027\n",
      "R2 Score for Feature 273: -0.0001\n",
      "R2 Score for Feature 274: -0.0023\n",
      "R2 Score for Feature 275: -0.0015\n",
      "R2 Score for Feature 276: -0.0001\n",
      "R2 Score for Feature 277: -0.0000\n",
      "R2 Score for Feature 278: -0.0000\n",
      "R2 Score for Feature 279: -0.0019\n",
      "R2 Score for Feature 280: -0.0011\n",
      "R2 Score for Feature 281: -0.0076\n",
      "R2 Score for Feature 282: -0.0044\n",
      "R2 Score for Feature 283: -0.0116\n",
      "R2 Score for Feature 284: -0.0015\n",
      "R2 Score for Feature 285: -0.0000\n",
      "R2 Score for Feature 286: -0.0026\n",
      "R2 Score for Feature 287: -0.0181\n",
      "R2 Score for Feature 288: -0.0004\n",
      "R2 Score for Feature 289: -0.0013\n",
      "R2 Score for Feature 290: -0.0010\n",
      "R2 Score for Feature 291: -0.0014\n",
      "R2 Score for Feature 292: -0.0004\n",
      "R2 Score for Feature 293: -0.0017\n",
      "R2 Score for Feature 294: -0.0001\n",
      "R2 Score for Feature 295: -0.0006\n",
      "R2 Score for Feature 296: -0.0009\n",
      "R2 Score for Feature 297: -0.0002\n",
      "R2 Score for Feature 298: -0.0084\n",
      "R2 Score for Feature 299: -0.0000\n",
      "R2 Score for Feature 300: -0.0039\n",
      "R2 Score for Feature 301: -0.0003\n",
      "R2 Score for Feature 302: -0.0009\n",
      "R2 Score for Feature 303: -0.0081\n",
      "R2 Score for Feature 304: -0.0047\n",
      "R2 Score for Feature 305: -0.0000\n",
      "R2 Score for Feature 306: -0.0012\n",
      "R2 Score for Feature 307: -0.0014\n",
      "R2 Score for Feature 308: -0.0001\n",
      "R2 Score for Feature 309: -0.0074\n",
      "R2 Score for Feature 310: -0.0030\n",
      "R2 Score for Feature 311: -0.0024\n",
      "R2 Score for Feature 312: -0.0014\n",
      "R2 Score for Feature 313: -0.0000\n",
      "R2 Score for Feature 314: -0.0005\n",
      "R2 Score for Feature 315: -0.0014\n",
      "R2 Score for Feature 316: -0.0094\n",
      "R2 Score for Feature 317: -0.0001\n",
      "R2 Score for Feature 318: -0.0004\n",
      "R2 Score for Feature 319: -0.0012\n",
      "R2 Score for Feature 320: -0.0055\n",
      "R2 Score for Feature 321: -0.0050\n",
      "R2 Score for Feature 322: -0.0003\n",
      "R2 Score for Feature 323: -0.0063\n",
      "R2 Score for Feature 324: -0.0004\n",
      "R2 Score for Feature 325: -0.0046\n",
      "R2 Score for Feature 326: -0.0151\n",
      "R2 Score for Feature 327: -0.0024\n",
      "R2 Score for Feature 328: -0.0009\n",
      "R2 Score for Feature 329: -0.0021\n",
      "R2 Score for Feature 330: -0.0004\n",
      "R2 Score for Feature 331: -0.0012\n",
      "R2 Score for Feature 332: -0.0004\n",
      "R2 Score for Feature 333: -0.0014\n",
      "R2 Score for Feature 334: -0.0019\n",
      "R2 Score for Feature 335: -0.0002\n",
      "R2 Score for Feature 336: -0.0029\n",
      "R2 Score for Feature 337: -0.0009\n",
      "R2 Score for Feature 338: -0.0030\n",
      "R2 Score for Feature 339: -0.0057\n",
      "R2 Score for Feature 340: -0.0005\n",
      "R2 Score for Feature 341: -0.0051\n",
      "R2 Score for Feature 342: -0.0015\n",
      "R2 Score for Feature 343: -0.0001\n",
      "R2 Score for Feature 344: -0.0027\n",
      "R2 Score for Feature 345: -0.0070\n",
      "R2 Score for Feature 346: -0.0019\n",
      "R2 Score for Feature 347: -0.0031\n",
      "R2 Score for Feature 348: -0.0008\n",
      "R2 Score for Feature 349: -0.0004\n",
      "R2 Score for Feature 350: -0.0075\n",
      "R2 Score for Feature 351: -0.0015\n",
      "R2 Score for Feature 352: -0.0118\n",
      "R2 Score for Feature 353: -0.0000\n",
      "R2 Score for Feature 354: -0.0197\n",
      "R2 Score for Feature 355: -0.0094\n",
      "R2 Score for Feature 356: -0.0043\n",
      "R2 Score for Feature 357: -0.0010\n",
      "R2 Score for Feature 358: -0.0002\n",
      "R2 Score for Feature 359: -0.0110\n",
      "R2 Score for Feature 360: -0.0002\n",
      "R2 Score for Feature 361: -0.0005\n",
      "R2 Score for Feature 362: -0.0000\n",
      "R2 Score for Feature 363: -0.0010\n",
      "R2 Score for Feature 364: -0.0061\n",
      "R2 Score for Feature 365: -0.0021\n",
      "R2 Score for Feature 366: -0.0004\n",
      "R2 Score for Feature 367: -0.0005\n",
      "R2 Score for Feature 368: -0.0034\n",
      "R2 Score for Feature 369: -0.0077\n",
      "R2 Score for Feature 370: -0.0063\n",
      "R2 Score for Feature 371: -0.0000\n",
      "R2 Score for Feature 372: -0.0001\n",
      "R2 Score for Feature 373: -0.0003\n",
      "R2 Score for Feature 374: -0.0005\n",
      "R2 Score for Feature 375: -0.0040\n",
      "R2 Score for Feature 376: -0.0038\n",
      "R2 Score for Feature 377: -0.0019\n",
      "R2 Score for Feature 378: -0.0001\n",
      "R2 Score for Feature 379: -0.0000\n",
      "R2 Score for Feature 380: -0.0006\n",
      "R2 Score for Feature 381: -0.0083\n",
      "R2 Score for Feature 382: -0.0001\n",
      "R2 Score for Feature 383: -0.0000\n",
      "R2 Score for Feature 384: -0.0051\n",
      "R2 Score for Feature 385: -0.0027\n",
      "R2 Score for Feature 386: -0.0093\n",
      "R2 Score for Feature 387: -0.0000\n",
      "R2 Score for Feature 388: -0.0008\n",
      "R2 Score for Feature 389: -0.0007\n",
      "R2 Score for Feature 390: -0.0006\n",
      "R2 Score for Feature 391: -0.0001\n",
      "R2 Score for Feature 392: -0.0005\n",
      "R2 Score for Feature 393: -0.0052\n",
      "R2 Score for Feature 394: -0.0007\n",
      "R2 Score for Feature 395: -0.0065\n",
      "R2 Score for Feature 396: -0.0000\n",
      "R2 Score for Feature 397: -0.0000\n",
      "R2 Score for Feature 398: -0.0088\n",
      "R2 Score for Feature 399: -0.0004\n",
      "R2 Score for Feature 400: -0.0006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.007697582244873047,\n",
       " -0.0003352165222167969,\n",
       " -0.00260770320892334,\n",
       " -0.0001093149185180664,\n",
       " -0.000823974609375,\n",
       " -0.0025768280029296875,\n",
       " -0.00043833255767822266,\n",
       " -0.0003472566604614258,\n",
       " -0.00034332275390625,\n",
       " -7.414817810058594e-05,\n",
       " -0.0005167722702026367,\n",
       " -0.008720040321350098,\n",
       " -0.000408172607421875,\n",
       " -0.008693814277648926,\n",
       " -0.00012505054473876953,\n",
       " -0.000828862190246582,\n",
       " -5.173683166503906e-05,\n",
       " -0.0031354427337646484,\n",
       " -4.124641418457031e-05,\n",
       " -0.002390265464782715,\n",
       " -4.088878631591797e-05,\n",
       " -0.00035381317138671875,\n",
       " -0.0011425018310546875,\n",
       " -0.001547098159790039,\n",
       " -0.007055401802062988,\n",
       " -0.0005979537963867188,\n",
       " -0.0038323402404785156,\n",
       " -0.007278323173522949,\n",
       " -0.0066986083984375,\n",
       " -1.1920928955078125e-07,\n",
       " -0.005735993385314941,\n",
       " -0.0056226253509521484,\n",
       " -0.0013349056243896484,\n",
       " -0.0014873743057250977,\n",
       " -0.0029587745666503906,\n",
       " -0.005314469337463379,\n",
       " -4.172325134277344e-06,\n",
       " -0.00012576580047607422,\n",
       " -0.006925702095031738,\n",
       " -0.0025686025619506836,\n",
       " -2.5987625122070312e-05,\n",
       " -0.0012879371643066406,\n",
       " -0.0026875734329223633,\n",
       " -3.540515899658203e-05,\n",
       " -0.0009853839874267578,\n",
       " -0.0003829002380371094,\n",
       " -0.01427757740020752,\n",
       " -0.0010150671005249023,\n",
       " -0.00011444091796875,\n",
       " -0.001473546028137207,\n",
       " -0.0020438432693481445,\n",
       " -0.011081814765930176,\n",
       " -0.0028890371322631836,\n",
       " -0.007282733917236328,\n",
       " -6.318092346191406e-06,\n",
       " -0.0001062154769897461,\n",
       " -0.0026979446411132812,\n",
       " -0.0020494461059570312,\n",
       " -9.524822235107422e-05,\n",
       " -0.001466989517211914,\n",
       " -0.0032197237014770508,\n",
       " -0.0042809247970581055,\n",
       " -0.007952213287353516,\n",
       " -0.001637578010559082,\n",
       " -0.0005068778991699219,\n",
       " -0.005434751510620117,\n",
       " -0.0004922151565551758,\n",
       " -0.019604206085205078,\n",
       " -0.007991552352905273,\n",
       " -0.009564042091369629,\n",
       " -0.005075931549072266,\n",
       " -0.0007598400115966797,\n",
       " -0.001955747604370117,\n",
       " -0.00023186206817626953,\n",
       " -5.4836273193359375e-06,\n",
       " -0.0015170574188232422,\n",
       " -0.0017791986465454102,\n",
       " -0.0015944242477416992,\n",
       " -0.0015120506286621094,\n",
       " -0.002031683921813965,\n",
       " -0.006095290184020996,\n",
       " -2.682209014892578e-05,\n",
       " -0.004299521446228027,\n",
       " -0.0011277198791503906,\n",
       " -0.0006003379821777344,\n",
       " -0.03552365303039551,\n",
       " -0.00030922889709472656,\n",
       " -0.007387638092041016,\n",
       " -0.0024203062057495117,\n",
       " -0.0031626224517822266,\n",
       " -0.009917497634887695,\n",
       " -0.0014445781707763672,\n",
       " -0.00021123886108398438,\n",
       " -0.005294203758239746,\n",
       " -2.5153160095214844e-05,\n",
       " -0.003411412239074707,\n",
       " -0.0008831024169921875,\n",
       " -0.007080435752868652,\n",
       " -0.00031960010528564453,\n",
       " -0.0010290145874023438,\n",
       " -0.0047130584716796875,\n",
       " -0.0003902912139892578,\n",
       " -0.00023508071899414062,\n",
       " -3.1113624572753906e-05,\n",
       " -0.0005642175674438477,\n",
       " -0.0076569318771362305,\n",
       " -0.0010254383087158203,\n",
       " -0.0019370317459106445,\n",
       " -0.0013821125030517578,\n",
       " -0.002432584762573242,\n",
       " -3.993511199951172e-05,\n",
       " -0.0025843381881713867,\n",
       " -0.0010515451431274414,\n",
       " -0.008583903312683105,\n",
       " -0.0026345252990722656,\n",
       " -4.756450653076172e-05,\n",
       " -0.0004228353500366211,\n",
       " -0.002962350845336914,\n",
       " -0.0064514875411987305,\n",
       " -0.00416254997253418,\n",
       " -0.004995942115783691,\n",
       " -0.0005882978439331055,\n",
       " -8.344650268554688e-07,\n",
       " -0.00021731853485107422,\n",
       " -0.007887005805969238,\n",
       " -0.00011813640594482422,\n",
       " -0.0026307106018066406,\n",
       " -0.006762027740478516,\n",
       " -0.010656476020812988,\n",
       " -0.0010929107666015625,\n",
       " -0.01408529281616211,\n",
       " -0.0003477334976196289,\n",
       " -0.001687169075012207,\n",
       " -0.0023964643478393555,\n",
       " -0.003160834312438965,\n",
       " -0.0034760236740112305,\n",
       " -6.639957427978516e-05,\n",
       " -0.001621842384338379,\n",
       " -0.006645083427429199,\n",
       " -0.003884434700012207,\n",
       " -0.001765131950378418,\n",
       " -0.0008701086044311523,\n",
       " -0.004448294639587402,\n",
       " -0.0024080276489257812,\n",
       " -0.004442691802978516,\n",
       " -0.007424473762512207,\n",
       " -4.887580871582031e-06,\n",
       " -0.0001958608627319336,\n",
       " -0.0015535354614257812,\n",
       " -0.002995729446411133,\n",
       " -0.0018674135208129883,\n",
       " -0.003233671188354492,\n",
       " -0.002572774887084961,\n",
       " -0.0027761459350585938,\n",
       " -0.003736734390258789,\n",
       " -0.00660860538482666,\n",
       " -0.0008065700531005859,\n",
       " -0.0015685558319091797,\n",
       " -0.0002046823501586914,\n",
       " -0.022026419639587402,\n",
       " -0.00362241268157959,\n",
       " -0.003246307373046875,\n",
       " -0.004362225532531738,\n",
       " -0.002418994903564453,\n",
       " -0.001983642578125,\n",
       " -0.0016620159149169922,\n",
       " -0.00037670135498046875,\n",
       " -6.854534149169922e-05,\n",
       " -0.005770564079284668,\n",
       " -0.0005916357040405273,\n",
       " -0.004858613014221191,\n",
       " -6.961822509765625e-05,\n",
       " -0.013894438743591309,\n",
       " -0.004867076873779297,\n",
       " -0.004660964012145996,\n",
       " -0.0002530813217163086,\n",
       " -0.003962278366088867,\n",
       " -0.00030744075775146484,\n",
       " -0.0021005868911743164,\n",
       " -0.002050638198852539,\n",
       " -0.0022472143173217773,\n",
       " -0.005420088768005371,\n",
       " -5.960464477539062e-07,\n",
       " -0.00014591217041015625,\n",
       " -0.001681208610534668,\n",
       " -0.00010943412780761719,\n",
       " -0.023383259773254395,\n",
       " -0.008309006690979004,\n",
       " -0.00842297077178955,\n",
       " -0.0010732412338256836,\n",
       " -0.00010287761688232422,\n",
       " -0.011492133140563965,\n",
       " -8.022785186767578e-05,\n",
       " -0.01223456859588623,\n",
       " -4.76837158203125e-07,\n",
       " -0.004968762397766113,\n",
       " -0.00011217594146728516,\n",
       " -0.007733941078186035,\n",
       " -0.0006984472274780273,\n",
       " -0.0006313323974609375,\n",
       " -5.543231964111328e-05,\n",
       " -0.0002841949462890625,\n",
       " -0.000874638557434082,\n",
       " -0.001277923583984375,\n",
       " -0.0014160871505737305,\n",
       " -0.0018187761306762695,\n",
       " -0.0005822181701660156,\n",
       " -0.00044667720794677734,\n",
       " -0.00037658214569091797,\n",
       " -0.0026988983154296875,\n",
       " -0.014320015907287598,\n",
       " -0.0005934238433837891,\n",
       " -1.9431114196777344e-05,\n",
       " -4.494190216064453e-05,\n",
       " -0.003035306930541992,\n",
       " -5.245208740234375e-06,\n",
       " -0.0009920597076416016,\n",
       " -0.00042808055877685547,\n",
       " -0.0017459392547607422,\n",
       " -0.001554727554321289,\n",
       " -0.0037919282913208008,\n",
       " -0.002616405487060547,\n",
       " -6.23464584350586e-05,\n",
       " -0.0019392967224121094,\n",
       " -0.0004481077194213867,\n",
       " -1.1920928955078125e-07,\n",
       " -0.002431154251098633,\n",
       " -8.344650268554688e-05,\n",
       " -5.054473876953125e-05,\n",
       " -0.00012063980102539062,\n",
       " -0.0071097612380981445,\n",
       " -0.000247955322265625,\n",
       " -0.005311250686645508,\n",
       " -0.002116680145263672,\n",
       " -0.0046585798263549805,\n",
       " -0.00021445751190185547,\n",
       " -0.007928133010864258,\n",
       " -0.014316558837890625,\n",
       " -6.306171417236328e-05,\n",
       " -0.005478858947753906,\n",
       " -0.0031986236572265625,\n",
       " -0.0004030466079711914,\n",
       " -0.0031299591064453125,\n",
       " -0.0032100677490234375,\n",
       " -0.0005904436111450195,\n",
       " -0.011631608009338379,\n",
       " -0.0017855167388916016,\n",
       " -0.0032052993774414062,\n",
       " -0.0006297826766967773,\n",
       " -0.0009645223617553711,\n",
       " -0.0039556026458740234,\n",
       " -2.1219253540039062e-05,\n",
       " -0.0036662817001342773,\n",
       " -0.0134507417678833,\n",
       " -0.0010303258895874023,\n",
       " -0.010560035705566406,\n",
       " -0.0017328262329101562,\n",
       " -0.0017616748809814453,\n",
       " -0.0014033317565917969,\n",
       " -0.00972890853881836,\n",
       " -0.005108833312988281,\n",
       " -0.004126310348510742,\n",
       " -0.0001672506332397461,\n",
       " -0.011166930198669434,\n",
       " -0.0004925727844238281,\n",
       " -1.3828277587890625e-05,\n",
       " -0.0012410879135131836,\n",
       " -0.016940951347351074,\n",
       " -0.014230966567993164,\n",
       " -0.0010672807693481445,\n",
       " -0.00045490264892578125,\n",
       " -0.0006203651428222656,\n",
       " -0.002731800079345703,\n",
       " -0.00013589859008789062,\n",
       " -0.0023268461227416992,\n",
       " -0.001475214958190918,\n",
       " -0.00011813640594482422,\n",
       " -4.76837158203125e-06,\n",
       " -1.2755393981933594e-05,\n",
       " -0.0019162893295288086,\n",
       " -0.0011063814163208008,\n",
       " -0.0076225996017456055,\n",
       " -0.004392743110656738,\n",
       " -0.0115889310836792,\n",
       " -0.0015244483947753906,\n",
       " -9.5367431640625e-07,\n",
       " -0.0025527477264404297,\n",
       " -0.01808035373687744,\n",
       " -0.0003821849822998047,\n",
       " -0.0013229846954345703,\n",
       " -0.0009642839431762695,\n",
       " -0.0013513565063476562,\n",
       " -0.00043582916259765625,\n",
       " -0.0017033815383911133,\n",
       " -6.222724914550781e-05,\n",
       " -0.0006461143493652344,\n",
       " -0.0009464025497436523,\n",
       " -0.0001806020736694336,\n",
       " -0.008428096771240234,\n",
       " -1.1920928955078125e-07,\n",
       " -0.003946900367736816,\n",
       " -0.0002856254577636719,\n",
       " -0.000910639762878418,\n",
       " -0.008100271224975586,\n",
       " -0.004677176475524902,\n",
       " -4.76837158203125e-07,\n",
       " -0.0011835098266601562,\n",
       " -0.0014039278030395508,\n",
       " -9.059906005859375e-05,\n",
       " -0.007398843765258789,\n",
       " -0.0029768943786621094,\n",
       " -0.0024384260177612305,\n",
       " -0.0013800859451293945,\n",
       " -1.5497207641601562e-06,\n",
       " -0.0005161762237548828,\n",
       " -0.0013583898544311523,\n",
       " -0.009433746337890625,\n",
       " -5.054473876953125e-05,\n",
       " -0.0004177093505859375,\n",
       " -0.0011942386627197266,\n",
       " -0.005486845970153809,\n",
       " -0.004990220069885254,\n",
       " -0.00025725364685058594,\n",
       " -0.006270170211791992,\n",
       " -0.00042235851287841797,\n",
       " -0.004567146301269531,\n",
       " -0.015082597732543945,\n",
       " -0.0023784637451171875,\n",
       " -0.0008643865585327148,\n",
       " -0.0021309852600097656,\n",
       " -0.00041735172271728516,\n",
       " -0.0011669397354125977,\n",
       " -0.0003724098205566406,\n",
       " -0.001416921615600586,\n",
       " -0.0018886327743530273,\n",
       " -0.000202178955078125,\n",
       " -0.002867460250854492,\n",
       " -0.0008769035339355469,\n",
       " -0.0029883384704589844,\n",
       " -0.005688190460205078,\n",
       " -0.0005499124526977539,\n",
       " -0.005110979080200195,\n",
       " -0.0014933347702026367,\n",
       " -0.00010991096496582031,\n",
       " -0.0027371644973754883,\n",
       " -0.007005810737609863,\n",
       " -0.0018541812896728516,\n",
       " -0.003070831298828125,\n",
       " -0.0007642507553100586,\n",
       " -0.0003654956817626953,\n",
       " -0.007503151893615723,\n",
       " -0.0014592409133911133,\n",
       " -0.011813759803771973,\n",
       " -2.753734588623047e-05,\n",
       " -0.019650578498840332,\n",
       " -0.009351372718811035,\n",
       " -0.004262566566467285,\n",
       " -0.0010014772415161133,\n",
       " -0.00022780895233154297,\n",
       " -0.010959744453430176,\n",
       " -0.0001634359359741211,\n",
       " -0.0005112886428833008,\n",
       " -3.6954879760742188e-06,\n",
       " -0.001044631004333496,\n",
       " -0.006075382232666016,\n",
       " -0.00212252140045166,\n",
       " -0.0004222393035888672,\n",
       " -0.00047087669372558594,\n",
       " -0.0033721923828125,\n",
       " -0.00770723819732666,\n",
       " -0.006252169609069824,\n",
       " -2.586841583251953e-05,\n",
       " -0.0001283884048461914,\n",
       " -0.0002957582473754883,\n",
       " -0.0005347728729248047,\n",
       " -0.004045724868774414,\n",
       " -0.003776073455810547,\n",
       " -0.0018978118896484375,\n",
       " -9.143352508544922e-05,\n",
       " -1.33514404296875e-05,\n",
       " -0.0006273984909057617,\n",
       " -0.008291482925415039,\n",
       " -0.00011658668518066406,\n",
       " -5.960464477539062e-07,\n",
       " -0.005057811737060547,\n",
       " -0.0026503801345825195,\n",
       " -0.00925898551940918,\n",
       " -8.344650268554688e-07,\n",
       " -0.0008012056350708008,\n",
       " -0.0006778240203857422,\n",
       " -0.000620722770690918,\n",
       " -7.18832015991211e-05,\n",
       " -0.0005069971084594727,\n",
       " -0.0052253007888793945,\n",
       " -0.0007492303848266602,\n",
       " -0.006490588188171387,\n",
       " -4.172325134277344e-05,\n",
       " -7.3909759521484375e-06,\n",
       " -0.008782744407653809,\n",
       " -0.0003871917724609375,\n",
       " -0.0006399154663085938]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    scalers=scalers,\n",
    "    target_indices=target_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application with trading strategies ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
